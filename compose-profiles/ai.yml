services:
  ollama:
    image: ollama/ollama:latest
    container_name: ${PROJECT_NAME}-ollama
    restart: unless-stopped
    volumes:
      - ollama-data:/root/.ollama
    ports:
      - "${OLLAMA_PORT}:11434"
    networks:
      - backend-bridge-network

  ollama-pull:
    image: alpine:latest
    container_name: ${PROJECT_NAME}-ollama-pull
    restart: "no"
    depends_on:
      supabase-db-init:
        condition: service_completed_successfully
      ollama:
        condition: service_started
    networks:
      - backend-bridge-network
    environment:
      PGHOST: supabase-db
      PGPORT: 5432
      PGDATABASE: ${SUPABASE_DB_NAME}
      PGUSER: ${SUPABASE_DB_USER}
      PGPASSWORD: ${SUPABASE_DB_PASSWORD}
      OLLAMA_HOST_URL: http://ollama:11434
    volumes:
      - ./ollama-pull/scripts:/scripts
    entrypoint: ["/scripts/pull.sh"]

  comfyui-init:
    image: alpine:latest
    container_name: ${PROJECT_NAME}-comfyui-init
    restart: "no"
    depends_on:
      supabase-db-init:
        condition: service_completed_successfully
      ollama-pull:
        condition: service_completed_successfully
    networks:
      - backend-bridge-network
    environment:
      PGHOST: supabase-db
      PGPORT: 5432
      PGDATABASE: ${SUPABASE_DB_NAME}
      PGUSER: ${SUPABASE_DB_USER}
      PGPASSWORD: ${SUPABASE_DB_PASSWORD}
      COMFYUI_MODELS_PATH: /models
    volumes:
      - ./comfyui-init/scripts:/scripts
      - comfyui-models:/models
    entrypoint: ["/scripts/download_models.sh"]

  local-deep-researcher:
    build:
      context: ./local-deep-researcher
    container_name: ${PROJECT_NAME}-local-deep-researcher
    restart: unless-stopped
    depends_on:
      supabase-db-init:
        condition: service_completed_successfully
      ollama-pull:
        condition: service_completed_successfully
    environment:
      # Database connection for LLM config
      DATABASE_URL: postgresql://${SUPABASE_DB_USER}:${SUPABASE_DB_PASSWORD}@supabase-db:5432/${SUPABASE_DB_NAME}
      # Ollama connection (containerized)
      OLLAMA_BASE_URL: http://ollama:11434
      # Search configuration
      SEARCH_API: ${LOCAL_DEEP_RESEARCHER_SEARCH_API:-duckduckgo}
      MAX_WEB_RESEARCH_LOOPS: ${LOCAL_DEEP_RESEARCHER_LOOPS:-3}
    ports:
      - "${LOCAL_DEEP_RESEARCHER_PORT}:2024"
    volumes:
      - local-deep-researcher-data:/app/data
    networks:
      - backend-bridge-network

  n8n:
    image: n8nio/n8n:latest
    container_name: ${PROJECT_NAME}-n8n
    restart: unless-stopped
    depends_on:
      supabase-db-init:
        condition: service_completed_successfully
      ollama-pull:
        condition: service_completed_successfully
    environment:
      DB_TYPE: postgresdb
      DB_POSTGRESDB_HOST: supabase-db
      DB_POSTGRESDB_PORT: 5432
      DB_POSTGRESDB_DATABASE: ${SUPABASE_DB_NAME}
      DB_POSTGRESDB_USER: ${SUPABASE_DB_USER}
      DB_POSTGRESDB_PASSWORD: ${SUPABASE_DB_PASSWORD}
      DB_POSTGRESDB_SCHEMA: n8n
      DB_SCHEMA: n8n
      N8N_HOST: ${N8N_HOST:-localhost}
      N8N_PORT: 5678
      N8N_PROTOCOL: ${N8N_PROTOCOL:-http}
      NODE_ENV: production
      N8N_ENCRYPTION_KEY: ${N8N_ENCRYPTION_KEY}
      N8N_AUTH_ENABLED: ${N8N_AUTH_ENABLED:-true}
      N8N_BASIC_AUTH_ACTIVE: ${N8N_BASIC_AUTH_ACTIVE:-true}
      N8N_BASIC_AUTH_USER: ${N8N_BASIC_AUTH_USER}
      N8N_BASIC_AUTH_PASSWORD: ${N8N_BASIC_AUTH_PASSWORD}
      N8N_SECURE_COOKIE: false
      N8N_RUNNERS_ENABLED: true
      N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS: true
      NODE_OPTIONS: --max_old_space_size=512
      EXECUTIONS_MODE: ${N8N_EXECUTIONS_MODE:-queue}
      QUEUE_BULL_REDIS_HOST: redis
      QUEUE_BULL_REDIS_PORT: 6379
      QUEUE_BULL_REDIS_PASSWORD: ${REDIS_PASSWORD}
      QUEUE_BULL_REDIS_DB: 0
    ports:
      - "${N8N_PORT}:5678"
    volumes:
      - n8n-data:/home/node/.n8n
    networks:
      - backend-bridge-network

  comfyui:
    image: ghcr.io/ai-dock/comfyui:${COMFYUI_IMAGE_TAG:-v2-cpu-22.04-v0.2.7}
    platform: ${COMFYUI_PLATFORM:-linux/amd64}
    container_name: ${PROJECT_NAME}-comfyui
    restart: unless-stopped
    depends_on:
      supabase-db-init:
        condition: service_completed_successfully
      ollama-pull:
        condition: service_completed_successfully
      comfyui-init:
        condition: service_completed_successfully
      supabase-storage:
        condition: service_started
      redis:
        condition: service_started
    environment:
      - COMFYUI_ARGS=${COMFYUI_ARGS:---listen --cpu}
      - AUTO_UPDATE=${COMFYUI_AUTO_UPDATE:-false}
      - COMFYUI_PORT_HOST=8188
      - SUPABASE_URL=${COMFYUI_KONG_URL:-http://kong-api-gateway:8000}
      - SUPABASE_ANON_KEY=${SUPABASE_ANON_KEY}
      - SUPABASE_STORAGE_BUCKET=${COMFYUI_STORAGE_BUCKET:-comfyui-images}
      - COMFYUI_UPLOAD_TO_SUPABASE=${COMFYUI_UPLOAD_TO_SUPABASE:-true}
    ports:
      - "${COMFYUI_PORT}:8188"
    volumes:
      - comfyui-models:/opt/ComfyUI/models
      - comfyui-output:/opt/ComfyUI/output
      - comfyui-input:/opt/ComfyUI/input
      - comfyui-custom-nodes:/opt/ComfyUI/custom_nodes
    networks:
      - backend-bridge-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8188/system_stats"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    labels:
      - "com.genai.service=image-generation"
      - "com.genai.category=ai"
      - "com.genai.dependencies=ollama,supabase-storage"

volumes:
  ollama-data:
  local-deep-researcher-data:
  n8n-data:
  comfyui-models:
  comfyui-output:
  comfyui-input:
  comfyui-custom-nodes: